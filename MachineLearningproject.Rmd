---
title: "Machine Learning Project"
author: "Art"
date: "October 19, 2016"
output: html_document
---


#Machine Learning submission
The goal of this project is to look at a training data set that came from accelerometers on the belt, forearm, and dumbell of 6 participants which were asked to perform the exersizes correctly and incorrectly in 5 different ways. Using this data, we are to build a model and decide if the test set of 20 activities are doing the excersize corrrectly, or incorrectly. 
In general we will follow the following process. 
1. load the data into a data frame
2. Check for data problems such as: irrelevant columns, missing values, out of range, etc.
3. Prepare the data for processing
4. Split or partition the data into training and testing data frames
5. Make sure classe a factor variable. 
6. Create a data model from the training data using the train function
7. Use predict function to run the testing data through the model created by training data.
8. use the confusionmatrix to compare accuracy of model
9. predict the values from the 20 example observations and get the results
##About the Data set...

The training data used for this project are available here:
  
  [Link](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here:
  
  [link](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
Note: this pml-testing data contains 20 cases which we will use to predict if person is doing the exercise correctly.

The description for this project comes from this source: [link](http://groupware.les.inf.puc-rio.br/har)
Let's set up the environment.
```{r echo=FALSE}
# load the training and test data sets
# Load libraries necessary for processing... caret,randomforest, etc
library(caret)
library(randomForest)
library(base)
library(e1071)
set.seed(123)
```

The easiest way to  reduce processing time and increase accuracy is to reduce the number of variables and keep only relevant variables. To do this, I looked at the prediction data set(the dataset that has 20 rows).
Below are a list of steps in regard to the data cleansing process. 
1. When looking at how to process the training data, I first looked at the "Testing data", the set of 20 observations you need to predict from. As you are looking at the Testing data set, you notice that most of the colums are NAs. When we eliminate those columns, data seems to be rather clean. Kind of surprised really. 
2.  I will then get rid of the names, time and date fields, and finally, the #DIV/0! entries. As stated above, #DIV/0! text was resolved by eliminating the columns that had NA on the testset(20 observations to validate on)
3. I also saw no need to remove the  new_window= yes entries since the #DIV/0! entries were eliminated in step 4. I was originally going to remove the "new_window ==yes" entries but found no need to do so.


```{r}
#Read the 2 data sets
`trainingset` <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"))
`testset`     <- read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"))

#remove columns that are null on the training and test set
trainingcolnames = names(trainingset[,colSums(is.na(testset))==0])
#remove timestamps, new_window and subject name
newtrainingcolnames=trainingcolnames[-c(2,3,4,5,6)]
reducedtrain =trainingset[,newtrainingcolnames]
#The classe variable does not exist in the testset... remove it.
testset.trainingcolnames = trainingcolnames[trainingcolnames != "classe"]
reducedtestset =testset[,testset.trainingcolnames]

#partition the training set into the training set and testing set with a 60/40 split between training and testing data sets.
inTrain = createDataPartition(reducedtrain$classe, p = 3/5)[[1]]
training =reducedtrain[inTrain,]
testing =  reducedtrain[-inTrain,]

```


As identified using the trainControl funcion, I used K-fold cross validation (method = "repeatedcv"), using 5 folds (number=5), and repeated  Repeated random sub-sampling validations (repeats = 3)

We then used the train function to create the data model using random forests and the trainControl object.  Once we have the training model built, we can use the predict function to pump the training data into the model, and predict the results, which in theory, should be close to the actual results which are given by the "classe" variable. 
Once we have the predicted values, and the known values, we can use the confusion matrix to evaluate how accurate our model is.

About the confusionMatrix.
As explained by the previous statement, the confusion Matrix compares the predicted results (pred.test) and the actual results (testing$classe),using our test data which is 40% of our total number of observations. The confusionMatrix function then outputs the Sensitivity, Specificity, Accuracy, the 95% Confidence interfal and other details relating to how the actual values and the predicted values compare. This information is extremely useful. 


```{r}
#to tune the random forest, I used the trainControl function. 
#you can also use the expand.grid function but I didn't do it.
trc = trainControl(method= "repeatedcv", repeats = 3 , number = 5 )
#exp.grid = 
#Now we create a data model using the training data.
start.time =proc.time()
mod.rf = train(classe~., data = training, method="rf", trControl = trc)
processing.time = proc.time() - start.time

# After we created the model, we must now test the model using the 40% of the original data that we called reducedtest

#1. predicting outcomes through model
pred.test = predict(mod.rf, testing)
#now lets look at the result
summary(pred.test==testing$classe)
#WOW! That is a pretty accurate model! Only 1 false prediction. Lets learn more by using the confusionMatrix

#using a confusionMatrix, we can compare the actual data results iwth the data results we predicted
cm.result = confusionMatrix(pred.test,testing$classe)
cm.result$overall

cm.result

```
You can see the output of the confusionMatrix function above. The output of the confusion matrix is held in the cm.result variable.
Since we have great accuracy, better than .999, I can be quite confident that the 20 observation results, as displayed above, are quite accurate. 

```{r}

# Now we predict results for  20 observations
result.20 = predict(mod.rf, reducedtestset)
summary(result.20)

```

As you can se above, all 20 observations seem to be done "exactly according to the specification" or in other words, Class A observations. 
Observations classes are as follows: 


Class | Meaning
------|--------
A     | Done exactly to specification
B     | Throwing the elbows to the front 
C     | Lifting the dumbbell only halfway
D     | lowering the dumbbell only halfway
E     | Throwing the hips to the front


Submission details: 
Submission will consist of a link to a Github repo 
R markdown and compiled HTML file describing your analysis. 
Text of the writeup to < 2000 words 
Number of figures to be less than 5. 
Submit a repo with a gh-pages branch so the HTML page can be viewed online

